{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# ============================================\n# 1. INSTALL DEPENDENCIES\n# ============================================\n\nprint(\"Installing dependencies...\")\n!pip install -q -U pip\n!pip install -q -U \"transformers>=4.31.0\" \"huggingface_hub>=0.18.0\"\n!pip install -q git+https://github.com/csebuetnlp/normalizer\n!pip install -q evaluate seqeval datasets accelerate\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:25.150916Z",
     "iopub.execute_input": "2025-10-23T09:52:25.151729Z",
     "iopub.status.idle": "2025-10-23T09:52:35.310828Z",
     "shell.execute_reply.started": "2025-10-23T09:52:25.151694Z",
     "shell.execute_reply": "2025-10-23T09:52:35.309979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Installing dependencies...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 2. IMPORTS\n# ============================================\n\nprint(\"\\nImporting libraries...\")\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification\n)\nimport evaluate\nimport numpy as np\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:35.312646Z",
     "iopub.execute_input": "2025-10-23T09:52:35.312895Z",
     "iopub.status.idle": "2025-10-23T09:52:35.318377Z",
     "shell.execute_reply.started": "2025-10-23T09:52:35.312872Z",
     "shell.execute_reply": "2025-10-23T09:52:35.317561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nImporting libraries...\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 3. GPU CHECK\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"GPU DIAGNOSTIC CHECK\")\nprint(\"=\"*60)\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU count: {torch.cuda.device_count()}\")\n    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    device = torch.device(\"cuda\")\n    print(\"✓ GPU is available and will be used!\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"⚠️ WARNING: No GPU detected! Training will be SLOW on CPU\")\n    print(\"In Colab: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:35.319072Z",
     "iopub.execute_input": "2025-10-23T09:52:35.319315Z",
     "iopub.status.idle": "2025-10-23T09:52:35.338892Z",
     "shell.execute_reply.started": "2025-10-23T09:52:35.319299Z",
     "shell.execute_reply": "2025-10-23T09:52:35.338400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n============================================================\nGPU DIAGNOSTIC CHECK\n============================================================\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nGPU count: 2\nGPU name: Tesla T4\nGPU memory: 15.83 GB\n✓ GPU is available and will be used!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4. LOAD DATASETS\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING WIKIANN TELUGU DATASET (Training)\")\nprint(\"=\"*60)\ntrain_dataset = load_dataset(\"wikiann\", \"te\")\nprint(\"Telugu dataset for training:\")\nprint(train_dataset)\nprint(f\"\\nTrain size: {len(train_dataset['train'])}\")\nprint(f\"Validation size: {len(train_dataset['validation'])}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING WIKIANN BANGLA DATASET (Testing - Cross-lingual Transfer)\")\nprint(\"=\"*60)\ntest_dataset = load_dataset(\"wikiann\", \"bn\")\nprint(\"Bangla dataset for testing:\")\nprint(test_dataset)\nprint(f\"Test size: {len(test_dataset['test'])}\")\n\n# Use Telugu for training/validation, Bangla for test\ndataset = {\n    \"train\": train_dataset[\"train\"],\n    \"validation\": train_dataset[\"validation\"],\n    \"test\": test_dataset[\"test\"]\n}\n\nprint(\"\\nSample from Telugu training set:\")\nprint(dataset[\"train\"][0])\n\nlabel_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\nprint(f\"\\nNER Labels: {label_list}\")\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"SAMPLE TELUGU SENTENCES WITH TAGS (Training Data)\")\nprint(\"-\"*60)\nfor i in range(3):\n    tokens = dataset[\"train\"][i][\"tokens\"]\n    tags = [label_list[t] for t in dataset[\"train\"][i][\"ner_tags\"]]\n    print(f\"\\nSentence {i+1}:\")\n    print(\"Tokens:\", \" \".join(tokens))\n    print(\"Tags:  \", \" \".join(tags))\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"SAMPLE BANGLA SENTENCES (Test Data - Cross-lingual Evaluation)\")\nprint(\"-\"*60)\nfor i in range(2):\n    tokens = dataset[\"test\"][i][\"tokens\"]\n    tags = [label_list[t] for t in dataset[\"test\"][i][\"ner_tags\"]]\n    print(f\"\\nSentence {i+1}:\")\n    print(\"Tokens:\", \" \".join(tokens))\n    print(\"Tags:  \", \" \".join(tags))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:35.339839Z",
     "iopub.execute_input": "2025-10-23T09:52:35.340127Z",
     "iopub.status.idle": "2025-10-23T09:52:37.203070Z",
     "shell.execute_reply.started": "2025-10-23T09:52:35.340099Z",
     "shell.execute_reply": "2025-10-23T09:52:37.202315Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5. LOAD TOKENIZER — INDICBERT\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING TOKENIZER (INDICBERT)\")\nprint(\"=\"*60)\nMODEL_NAME = \"ai4bharat/IndicBERTv2-MLM-only\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:37.204906Z",
     "iopub.execute_input": "2025-10-23T09:52:37.205132Z",
     "iopub.status.idle": "2025-10-23T09:52:37.988331Z",
     "shell.execute_reply.started": "2025-10-23T09:52:37.205113Z",
     "shell.execute_reply": "2025-10-23T09:52:37.987632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n============================================================\nLOADING TOKENIZER (INDICBERT)\n============================================================\nTokenizer loaded: ai4bharat/IndicBERTv2-MLM-only\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6. TOKENIZATION FUNCTION\n# ============================================\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        padding=False,\n        max_length=128\n    )\n\n    all_labels = []\n    for i, labels in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = []\n        previous_word_idx = None\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(labels[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        all_labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = all_labels\n    return tokenized_inputs\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:37.989048Z",
     "iopub.execute_input": "2025-10-23T09:52:37.989328Z",
     "iopub.status.idle": "2025-10-23T09:52:37.994950Z",
     "shell.execute_reply.started": "2025-10-23T09:52:37.989309Z",
     "shell.execute_reply": "2025-10-23T09:52:37.994366Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7. TOKENIZE DATASET\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TOKENIZING DATASETS\")\nprint(\"=\"*60)\n\n# Tokenize each split separately\ntokenized_train = dataset[\"train\"].map(\n    tokenize_and_align_labels,\n    batched=True,\n    desc=\"Tokenizing Telugu train\"\n)\ntokenized_val = dataset[\"validation\"].map(\n    tokenize_and_align_labels,\n    batched=True,\n    desc=\"Tokenizing Telugu validation\"\n)\ntokenized_test = dataset[\"test\"].map(\n    tokenize_and_align_labels,\n    batched=True,\n    desc=\"Tokenizing Bangla test\"\n)\n\n# Remove unnecessary columns\ntokenized_train = tokenized_train.remove_columns([\"tokens\", \"ner_tags\", \"langs\"])\ntokenized_val = tokenized_val.remove_columns([\"tokens\", \"ner_tags\", \"langs\"])\ntokenized_test = tokenized_test.remove_columns([\"tokens\", \"ner_tags\", \"langs\"])\n\n# Create the tokenized_datasets dictionary\ntokenized_datasets = {\n    \"train\": tokenized_train,\n    \"validation\": tokenized_val,\n    \"test\": tokenized_test\n}\n\nprint(\"Tokenized datasets:\")\nprint(f\"  Train: {len(tokenized_datasets['train'])} samples\")\nprint(f\"  Validation: {len(tokenized_datasets['validation'])} samples\")\nprint(f\"  Test (Bangla): {len(tokenized_datasets['test'])} samples\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:37.995875Z",
     "iopub.execute_input": "2025-10-23T09:52:37.996252Z",
     "iopub.status.idle": "2025-10-23T09:52:38.233961Z",
     "shell.execute_reply.started": "2025-10-23T09:52:37.996227Z",
     "shell.execute_reply": "2025-10-23T09:52:38.233333Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 8. LOAD MODEL — INDICBERT\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING MODEL (INDICBERT)\")\nprint(\"=\"*60)\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=len(label_list),\n    id2label={i: label for i, label in enumerate(label_list)},\n    label2id={label: i for i, label in enumerate(label_list)}\n)\nmodel = model.to(device)\nprint(f\"Model loaded: {MODEL_NAME}\")\nprint(f\"Model device: {next(model.parameters()).device}\")\nprint(f\"Number of parameters: {model.num_parameters():,}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:38.234727Z",
     "iopub.execute_input": "2025-10-23T09:52:38.234978Z",
     "iopub.status.idle": "2025-10-23T09:52:40.601740Z",
     "shell.execute_reply.started": "2025-10-23T09:52:38.234949Z",
     "shell.execute_reply": "2025-10-23T09:52:40.601095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n============================================================\nLOADING MODEL (INDICBERT)\n============================================================\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai4bharat/IndicBERTv2-MLM-only and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Model loaded: ai4bharat/IndicBERTv2-MLM-only\nModel device: cuda:0\nNumber of parameters: 277,456,135\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 9. SETUP METRICS\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING METRICS\")\nprint(\"=\"*60)\nmetric = evaluate.load(\"seqeval\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:40.602547Z",
     "iopub.execute_input": "2025-10-23T09:52:40.602786Z",
     "iopub.status.idle": "2025-10-23T09:52:41.077905Z",
     "shell.execute_reply.started": "2025-10-23T09:52:40.602767Z",
     "shell.execute_reply": "2025-10-23T09:52:41.077376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n============================================================\nLOADING METRICS\n============================================================\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 10. DATA COLLATOR\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SETTING UP DATA COLLATOR\")\nprint(\"=\"*60)\ndata_collator = DataCollatorForTokenClassification(\n    tokenizer=tokenizer,\n    padding=True,\n    label_pad_token_id=-100\n)\nprint(\"Data collator created successfully!\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:41.078661Z",
     "iopub.execute_input": "2025-10-23T09:52:41.078914Z",
     "iopub.status.idle": "2025-10-23T09:52:41.083629Z",
     "shell.execute_reply.started": "2025-10-23T09:52:41.078889Z",
     "shell.execute_reply": "2025-10-23T09:52:41.082996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n============================================================\nSETTING UP DATA COLLATOR\n============================================================\nData collator created successfully!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 11. TRAINING ARGUMENTS\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SETTING UP TRAINING ARGUMENTS\")\nprint(\"=\"*60)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results-indicbert-telugu-ner\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    fp16=torch.cuda.is_available(),\n    push_to_hub=False,\n    save_total_limit=2,\n    report_to=\"none\",\n)\n\nprint(f\"Training device: {training_args.device}\")\nprint(f\"FP16 training: {training_args.fp16}\")\nprint(f\"Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"Number of epochs: {training_args.num_train_epochs}\")\nprint(\"\\n⚠️ NOTE: Training on TELUGU data, Testing on BANGLA data (Cross-lingual Transfer)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:41.084348Z",
     "iopub.execute_input": "2025-10-23T09:52:41.084574Z",
     "iopub.status.idle": "2025-10-23T09:52:41.133507Z",
     "shell.execute_reply.started": "2025-10-23T09:52:41.084550Z",
     "shell.execute_reply": "2025-10-23T09:52:41.132901Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 12. CREATE TRAINER\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING TRAINER\")\nprint(\"=\"*60)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nprint(\"Trainer created successfully!\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:41.134156Z",
     "iopub.execute_input": "2025-10-23T09:52:41.134847Z",
     "iopub.status.idle": "2025-10-23T09:52:41.163125Z",
     "shell.execute_reply.started": "2025-10-23T09:52:41.134820Z",
     "shell.execute_reply": "2025-10-23T09:52:41.162588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n============================================================\nCREATING TRAINER\n============================================================\nTrainer created successfully!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 13. START TRAINING\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\nprint(f\"This will take approximately 15–25 minutes on a T4 GPU\")\nprint(\"=\"*60 + \"\\n\")\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETED!\")\nprint(\"=\"*60)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T09:52:41.163825Z",
     "iopub.execute_input": "2025-10-23T09:52:41.163991Z",
     "iopub.status.idle": "2025-10-23T10:03:34.347405Z",
     "shell.execute_reply.started": "2025-10-23T09:52:41.163978Z",
     "shell.execute_reply": "2025-10-23T10:03:34.346592Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 3}.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n============================================================\nSTARTING TRAINING\n============================================================\nThis will take approximately 15–25 minutes on a T4 GPU\n============================================================\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1565/1565 10:52, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.210400</td>\n      <td>0.164587</td>\n      <td>0.911894</td>\n      <td>0.934959</td>\n      <td>0.923283</td>\n      <td>0.954598</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.095500</td>\n      <td>0.140970</td>\n      <td>0.936607</td>\n      <td>0.947606</td>\n      <td>0.942075</td>\n      <td>0.966521</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.089900</td>\n      <td>0.104335</td>\n      <td>0.957998</td>\n      <td>0.968383</td>\n      <td>0.963163</td>\n      <td>0.977299</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.028900</td>\n      <td>0.117683</td>\n      <td>0.957105</td>\n      <td>0.967480</td>\n      <td>0.962264</td>\n      <td>0.977069</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.025100</td>\n      <td>0.110342</td>\n      <td>0.964061</td>\n      <td>0.969286</td>\n      <td>0.966667</td>\n      <td>0.978675</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n============================================================\nTRAINING COMPLETED!\n============================================================\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 14. FINAL EVALUATION\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL EVALUATION ON TELUGU VALIDATION SET\")\nprint(\"=\"*60)\neval_results = trainer.evaluate()\nprint(\"\\nTelugu Validation Results:\")\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CROSS-LINGUAL EVALUATION ON BANGLA TEST SET\")\nprint(\"=\"*60)\nprint(\"⚠️ Testing cross-lingual transfer: Telugu-trained model on Bangla data\")\ntest_results = trainer.evaluate(tokenized_datasets[\"test\"])\nprint(\"\\nBangla Test Results (Cross-lingual Transfer):\")\nfor key, value in test_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRANSFER LEARNING ANALYSIS\")\nprint(\"=\"*60)\nprint(f\"Telugu → Telugu F1: {eval_results['eval_f1']:.4f}\")\nprint(f\"Telugu → Bangla F1: {test_results['eval_f1']:.4f}\")\ntransfer_gap = eval_results['eval_f1'] - test_results['eval_f1']\nprint(f\"Transfer Gap: {transfer_gap:.4f}\")\nif transfer_gap > 0:\n    print(\"✓ Model performs better on source language (expected)\")\nelse:\n    print(\"⚠️ Model performs better on target language (unexpected)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T10:03:34.351598Z",
     "iopub.execute_input": "2025-10-23T10:03:34.351952Z",
     "iopub.status.idle": "2025-10-23T10:03:50.876341Z",
     "shell.execute_reply.started": "2025-10-23T10:03:34.351921Z",
     "shell.execute_reply": "2025-10-23T10:03:50.875550Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 15. SAVE MODEL\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAVING FINAL MODEL\")\nprint(\"=\"*60)\ntrainer.save_model(\"./indicbert-telugu-ner-final\")\ntokenizer.save_pretrained(\"./indicbert-telugu-ner-final\")\nprint(\"Model saved to: ./indicbert-telugu-ner-final\")\nprint(\"This model was trained on Telugu and can be used for cross-lingual transfer to Bangla\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T10:03:50.877241Z",
     "iopub.execute_input": "2025-10-23T10:03:50.877606Z",
     "iopub.status.idle": "2025-10-23T10:03:53.588072Z",
     "shell.execute_reply.started": "2025-10-23T10:03:50.877580Z",
     "shell.execute_reply": "2025-10-23T10:03:53.587226Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import shutil\nimport os\n\n# Define the output zip file\noutput_filename = \"kaggle_working_dir.zip\"\n\n# Current working directory\ncurrent_dir = os.getcwd()\n\n# Create a zip of the current directory\nshutil.make_archive(\"kaggle_working_dir\", 'zip', current_dir)\n\nprint(f\"✅ ZIP file created: {output_filename}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T10:03:53.589016Z",
     "iopub.execute_input": "2025-10-23T10:03:53.589835Z",
     "iopub.status.idle": "2025-10-23T10:08:29.605456Z",
     "shell.execute_reply.started": "2025-10-23T10:03:53.589812Z",
     "shell.execute_reply": "2025-10-23T10:08:29.604724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "✅ ZIP file created: kaggle_working_dir.zip\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 16. TEST INFERENCE\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING INFERENCE\")\nprint(\"=\"*60)\n\ntest_sentence = dataset[\"test\"][0]\ntest_tokens = test_sentence[\"tokens\"]\nprint(f\"\\nTest sentence: {' '.join(test_tokens)}\")\n\ninputs = tokenizer(\n    test_tokens,\n    is_split_into_words=True,\n    return_tensors=\"pt\",\n    truncation=True,\n    padding=True\n).to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.argmax(outputs.logits, dim=2)\n\npredicted_labels = [label_list[p.item()] for p in predictions[0]]\nword_ids = inputs.word_ids()\n\nfinal_predictions = []\nprevious_word_idx = None\nfor word_idx, pred_label in zip(word_ids, predicted_labels):\n    if word_idx is not None and word_idx != previous_word_idx:\n        final_predictions.append(pred_label)\n        previous_word_idx = word_idx\n\nprint(\"\\nPredicted NER tags:\")\nfor token, pred_tag in zip(test_tokens, final_predictions):\n    print(f\"  {token:20s} -> {pred_tag}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALL DONE! 🎉\")\nprint(\"=\"*60)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-23T10:08:29.606285Z",
     "iopub.execute_input": "2025-10-23T10:08:29.606551Z",
     "iopub.status.idle": "2025-10-23T10:08:29.673950Z",
     "shell.execute_reply.started": "2025-10-23T10:08:29.606520Z",
     "shell.execute_reply": "2025-10-23T10:08:29.673368Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n============================================================\nTESTING INFERENCE\n============================================================\n\nTest sentence: উরুগুয়ে জাতীয় ফুটবল দল\n\nPredicted NER tags:\n  উরুগুয়ে             -> B-ORG\n  জাতীয়               -> I-ORG\n  ফুটবল                -> I-ORG\n  দল                   -> I-ORG\n\n============================================================\nALL DONE! 🎉\n============================================================\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 41
  }
 ]
}